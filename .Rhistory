length(which(kb$gatherings=="NA"))
length(which(kb$gatherings.original=="NA"))
print(citation("NLP"), bibtex=TRUE)
print(citation("openNLP"), bibtex=TRUE)
print(citation("cForest"), bibtex=TRUE)
print(citation("cforest"), bibtex=TRUE)
print(citation("party"), bibtex=TRUE)
library(cForest)
install.packages("cForest")
library(cForest)
install.packages("cforest")
library(cforest)
library(party)
session_info()
print(citation("party"), bibtex=TRUE)
print(citation("randomForest"), bibtex=TRUE)
make_pic_comparison_bars(filepath=outputpath,
inputfile_patterns = c("pos_trigrams_caret_whole_title.*_mtry10__measures.txt",
"pos_trigrams_caret_title_only.*_mtry10__measures.txt") ,
parameter_names = c("H", "Precision", "Recall"),
group_names = c("Whole title", "Title only"),
outputfile = "Qualification - POS trigrams",
main_title = "Qualification - POS trigrams",
sub_title = "Features: POS trigrams from the title, subtitle included or excluded",
x_title = "",
y_title = "",
legend_labels = c("H", "Precision", "Recall"),
#x_tick_labels = c("Main title only", "Whole title"),
#x_tick_breaks = c(1,2),
legend_title_parentheses=c(""))
# POS trigrams II
# check which mtry is the best
make_pic_comparison_bars(filepath=outputpath,
inputfile_patterns = c("pos_trigrams_whole_title_ntree250_mtry5_confusionMatrix_combined_no_cutoff.txt",
"pos_trigrams_whole_title_ntree250_mtry10_confusionMatrix_combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
group_names = c("Mtry 5", "Mtry 10"),
outputfile = "Qualification - POS trigrams and the effect of mtry",
main_title = "Qualification - POS trigrams and the effect of mtry",
sub_title = "Features: POS trigrams from the whole title",
x_title = "",
y_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
#x_tick_labels = c("Mtry 5", "Mtry 10"),
#x_tick_breaks = c(1,2),
legend_title_parentheses=c(""),
newschool = TRUE)
source("init.R")
make_pic_comparison_bars(filepath=outputpath,
inputfile_patterns = c("pos_trigrams_caret_whole_title.*_mtry10__measures.txt",
"pos_trigrams_caret_title_only.*_mtry10__measures.txt") ,
parameter_names = c("H", "Precision", "Recall"),
group_names = c("Whole title", "Title only"),
outputfile = "Qualification - POS trigrams",
main_title = "Qualification - POS trigrams",
sub_title = "Features: POS trigrams from the title, subtitle included or excluded",
x_title = "",
y_title = "",
legend_labels = c("H", "Precision", "Recall"),
#x_tick_labels = c("Main title only", "Whole title"),
#x_tick_breaks = c(1,2),
legend_title_parentheses=c(""))
# POS trigrams II
# check which mtry is the best
make_pic_comparison_bars(filepath=outputpath,
inputfile_patterns = c("pos_trigrams_whole_title_ntree250_mtry5_confusionMatrix_combined_no_cutoff.txt",
"pos_trigrams_whole_title_ntree250_mtry10_confusionMatrix_combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
group_names = c("Mtry 5", "Mtry 10"),
outputfile = "Qualification - POS trigrams and the effect of mtry",
main_title = "Qualification - POS trigrams and the effect of mtry",
sub_title = "Features: POS trigrams from the whole title",
x_title = "",
y_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
#x_tick_labels = c("Mtry 5", "Mtry 10"),
#x_tick_breaks = c(1,2),
legend_title_parentheses=c(""),
newschool = TRUE)
source("aggregate_aggregated_cm_newschool.R")
make_pic_comparison_bars(filepath=outputpath,
inputfile_patterns = c("pos_trigrams_caret_whole_title.*_mtry10__measures.txt",
"pos_trigrams_caret_title_only.*_mtry10__measures.txt") ,
parameter_names = c("H", "Precision", "Recall"),
group_names = c("Whole title", "Title only"),
outputfile = "Qualification - POS trigrams",
main_title = "Qualification - POS trigrams",
sub_title = "Features: POS trigrams from the title, subtitle included or excluded",
x_title = "",
y_title = "",
legend_labels = c("H", "Precision", "Recall"),
#x_tick_labels = c("Main title only", "Whole title"),
#x_tick_breaks = c(1,2),
legend_title_parentheses=c(""))
# POS trigrams II
# check which mtry is the best
make_pic_comparison_bars(filepath=outputpath,
inputfile_patterns = c("pos_trigrams_whole_title_ntree250_mtry5_confusionMatrix_combined_no_cutoff.txt",
"pos_trigrams_whole_title_ntree250_mtry10_confusionMatrix_combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
group_names = c("Mtry 5", "Mtry 10"),
outputfile = "Qualification - POS trigrams and the effect of mtry",
main_title = "Qualification - POS trigrams and the effect of mtry",
sub_title = "Features: POS trigrams from the whole title",
x_title = "",
y_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
#x_tick_labels = c("Mtry 5", "Mtry 10"),
#x_tick_breaks = c(1,2),
legend_title_parentheses=c(""),
newschool = TRUE)
source('~/GitHub/Gradu/make_pic_comparison_bars.R', encoding = 'UTF-8')
make_pic_comparison_bars(filepath=outputpath,
inputfile_patterns = c("pos_trigrams_caret_whole_title.*_mtry10__measures.txt",
"pos_trigrams_caret_title_only.*_mtry10__measures.txt") ,
parameter_names = c("H", "Precision", "Recall"),
group_names = c("Whole title", "Title only"),
outputfile = "Qualification - POS trigrams",
main_title = "Qualification - POS trigrams",
sub_title = "Features: POS trigrams from the title, subtitle included or excluded",
x_title = "",
y_title = "",
legend_labels = c("H", "Precision", "Recall"),
#x_tick_labels = c("Main title only", "Whole title"),
#x_tick_breaks = c(1,2),
legend_title_parentheses=c(""))
# POS trigrams II
# check which mtry is the best
make_pic_comparison_bars(filepath=outputpath,
inputfile_patterns = c("pos_trigrams_whole_title_ntree250_mtry5_confusionMatrix_combined_no_cutoff.txt",
"pos_trigrams_whole_title_ntree250_mtry10_confusionMatrix_combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
group_names = c("Mtry 5", "Mtry 10"),
outputfile = "Qualification - POS trigrams and the effect of mtry",
main_title = "Qualification - POS trigrams and the effect of mtry",
sub_title = "Features: POS trigrams from the whole title",
x_title = "",
y_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
#x_tick_labels = c("Mtry 5", "Mtry 10"),
#x_tick_breaks = c(1,2),
legend_title_parentheses=c(""),
newschool = TRUE)
make_pic_comparison_bars(filepath=outputpath,
inputfile_patterns = c("pos_caret_whole_title.*_mtry10__measures.txt",
"pos_trigrams_caret_whole_title.*_mtry10__measures.txt") ,
parameter_names = c("H", "Precision", "Recall"),
group_names = c("POS tags", "POS trigrams"),
outputfile = "POS tags versus POS trigrams",
main_title = "POS tags versus POS trigrams",
sub_title = "Mtry values 10, ntree 250",
x_title = "",
y_title = "",
legend_labels = c("H", "Precision", "Recall"),
#x_tick_labels = c("Main title only", "Whole title"),
#x_tick_breaks = c(1,2),
legend_title_parentheses=c(""))
source('~/GitHub/Gradu/make_all_pics.R')
make_pic_comparison_bars(filepath=outputpath,
inputfile_patterns = c("pos_caret_whole_title_ntree250_mtry10__measures.txt",
"pos_trigrams_caret_whole_title_ntree250_mtry10__measures.txt") ,
parameter_names = c("H", "Precision", "Recall"),
group_names = c("POS tags", "POS trigrams"),
outputfile = "POS tags versus POS trigrams",
main_title = "POS tags versus POS trigrams",
sub_title = "Mtry values 10, ntree 250",
x_title = "",
y_title = "",
legend_labels = c("H", "Precision", "Recall"),
#x_tick_labels = c("Main title only", "Whole title"),
#x_tick_breaks = c(1,2),
legend_title_parentheses=c(""))
source('~/GitHub/Gradu/init.R')
source('~/GitHub/Gradu/run_caret_rf.R')
source('~/GitHub/Gradu/make_all_pics.R')
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry((50)|(100)|(200))_ntree250_mtry5_.*combined_no_cutoff.txt",
"poetry((50)|(100)|(200))_whole_title_alt_ntree250_mtry5_.*combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of different methods in deciding the poetry words 25-300",
main_title = "Effect of different methods for deciding the poetry words, ntree = 250, mtry=5",
sub_title = "Features: 25, 50, 100, 200, 300 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("25", "50", "100", "200", "300"),
x_tick_breaks = c(25, 50, 100, 200, 300),
legend_title_parentheses=c("Method 1", "Method 2"))
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry((50)|(100)|(200))_ntree250_mtry5_.*combined_no_cutoff.txt",
"poetry((50)|(100)|(200))_whole_title_alt_ntree250_mtry5_.*combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of different methods in deciding the poetry words 50-200",
main_title = "Effect of different methods for deciding the poetry words, ntree = 250, mtry=5",
sub_title = "Features: 25, 50, 100, 200, 300 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("50", "100", "200"),
x_tick_breaks = c(50, 100, 200),
legend_title_parentheses=c("Method 1", "Method 2"))
feats_pos_100per50_whole_title <- readRDS(paste0(bu_path, "/feats_pos_trigrams_100per50.RDS"))
feats_pos_100per50_whole_title <- NULL
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry100_ntree250_mtry.*combined_no_cutoff.txt",
"poetry100_ntree500_mtry.*combined_no_cutoff.txt",
"poetry100_ntree750_mtry.*combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of mtry",
main_title = "Effect of mtry",
sub_title = "Features: 100 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("5", "10", "15"),
x_tick_breaks = c(5, 10, 15),
legend_title_parentheses=c("Ntree=250", "Ntree=500", "Ntree=750"))
sink()
debugSource('~/GitHub/Gradu/make_pic_comparison_lines.R')
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry100_ntree250_mtry.*combined_no_cutoff.txt",
"poetry100_ntree500_mtry.*combined_no_cutoff.txt",
"poetry100_ntree750_mtry.*combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of mtry",
main_title = "Effect of mtry",
sub_title = "Features: 100 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("5", "10", "15"),
x_tick_breaks = c(5, 10, 15),
legend_title_parentheses=c("Ntree=250", "Ntree=500", "Ntree=750"))
source('~/GitHub/Gradu/make_pic_comparison_lines.R')
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry100_ntree250_mtry.*combined_no_cutoff.txt",
"poetry100_ntree500_mtry.*combined_no_cutoff.txt",
"poetry100_ntree750_mtry.*combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of mtry",
main_title = "Effect of mtry",
sub_title = "Features: 100 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("5", "10", "15"),
x_tick_breaks = c(5, 10, 15),
legend_title_parentheses=c("Ntree=250", "Ntree=500", "Ntree=750"))
source('~/GitHub/Gradu/make_pic_comparison_lines.R')
debugSource('~/GitHub/Gradu/aggregate_aggregated_cm.R')
source('~/GitHub/Gradu/make_pic_comparison_lines.R')
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry100_ntree250_mtry.*combined_no_cutoff.txt",
"poetry100_ntree500_mtry.*combined_no_cutoff.txt",
"poetry100_ntree750_mtry.*combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of mtry",
main_title = "Effect of mtry",
sub_title = "Features: 100 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("5", "10", "15"),
x_tick_breaks = c(5, 10, 15),
legend_title_parentheses=c("Ntree=250", "Ntree=500", "Ntree=750"))
files
filenames
ret
debugSource('~/GitHub/Gradu/aggregate_aggregated_cm.R')
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry100_ntree250_mtry.*combined_no_cutoff.txt",
"poetry100_ntree500_mtry.*combined_no_cutoff.txt",
"poetry100_ntree750_mtry.*combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of mtry",
main_title = "Effect of mtry",
sub_title = "Features: 100 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("5", "10", "15"),
x_tick_breaks = c(5, 10, 15),
legend_title_parentheses=c("Ntree=250", "Ntree=500", "Ntree=750"))
source('~/GitHub/Gradu/aggregate_aggregated_cm.R')
source('~/GitHub/Gradu/make_pic_comparison_lines.R')
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry100_ntree250_mtry.*combined_no_cutoff.txt",
"poetry100_ntree500_mtry.*combined_no_cutoff.txt",
"poetry100_ntree750_mtry.*combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of mtry",
main_title = "Effect of mtry",
sub_title = "Features: 100 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("5", "10", "15"),
x_tick_breaks = c(5, 10, 15),
legend_title_parentheses=c("Ntree=250", "Ntree=500", "Ntree=750"))
cm <- read.delim2("C:/Users/Hege/Opiskelu/Kurssit/Gradu/output/poetry100_ntree250_mtry5_confusionMatrix_combined_no_cutoff.txt")
cm <- read.delim2("C:/Users/Hege/Opiskelu/Kurssit/Gradu/output/poetry100_ntree250_mtry5_confusionMatrix_combined_no_cutoff.txt", sep="")
cm <- read.delim2("C:/Users/Hege/Opiskelu/Kurssit/Gradu/output/poetry100_ntree250_mtry5_confusionMatrix_combined_no_cutoff.txt", sep=" ")
cm$X.7
cm[["total"]]
cm[["total"]][1]
cm["total"]
cm <- read.delim2("C:/Users/Hege/Opiskelu/Kurssit/Gradu/output/poetry100_ntree250_mtry5_confusionMatrix_combined_no_cutoff.txt", sep="\t")
cm
cm[["total"]]
cm[["total"]][1]
unlist(str_split(as.character(cm[["total"]][1]), " +"))[7]
source('~/GitHub/Gradu/aggregate_aggregated_cm.R')
source('~/GitHub/Gradu/make_pic_comparison_lines.R')
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry100_ntree250_mtry.*combined_no_cutoff.txt",
"poetry100_ntree500_mtry.*combined_no_cutoff.txt",
"poetry100_ntree750_mtry.*combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of mtry",
main_title = "Effect of mtry",
sub_title = "Features: 100 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("5", "10", "15"),
x_tick_breaks = c(5, 10, 15),
legend_title_parentheses=c("Ntree=250", "Ntree=500", "Ntree=750"))
list.files(path = outputpath, pattern = "poetry[0-9]+_ntree250_mtry5")
list.files(path = outputpath, pattern = "poetry[0-9]+_ntree250_mtry5.*no_cutoff.txt")
list.files(path = outputpath, pattern = "poetry[0-9]+_ntree250_mtry10.*no_cutoff.txt")
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry[0-9]+_ntree250_mtry10.*combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of BOW size",
main_title = "Effect of BOW size: Method 1",
sub_title = "Features: 25, 50, 100, 200, 300 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("25", "50", "100", "200", "300"),
x_tick_breaks = c(25, 50, 100, 200, 300),
legend_title_parentheses=c(""))
list.files(path = outputpath, pattern = "poetry[0-9]+_alt_ntree250_mtry10.*no_cutoff.txt")
list.files(path = outputpath, pattern = "poetry_alt_[0-9]+_ntree250_mtry10.*no_cutoff.txt")
list.files(path = outputpath, pattern = "poetry[0-9]+_ntree250_mtry10_alt.*no_cutoff.txt")
list.files(path = outputpath, pattern = "poetry[0-9]+_whole_title_alt_ntree250_mtry10.*no_cutoff.txt")
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry[0-9]+_whole_title_alt_ntree250_mtry10.*no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of BOW size alt",
main_title = "Effect of BOW size: Method 2",
sub_title = "Features: 25, 50, 100, 200, 300 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("25", "50", "100", "200", "300"),
x_tick_breaks = c(25, 50, 100, 200, 300),
legend_title_parentheses=c(""))
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry[0-9]+_title_only_alt_ntree250_mtry10.*no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of BOW size alt title only",
main_title = "Effect of BOW size: Method 3",
sub_title = "Features: 25, 50, 100, 200, 300 most common words in poetry book titles",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("25", "50", "100", "200", "300"),
x_tick_breaks = c(25, 50, 100, 200, 300),
legend_title_parentheses=c(""))
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry[0-9]+_ntree250_mtry10.*combined_no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of BOW size",
main_title = "Effect of BOW size: Method 1",
sub_title = "Features: 25, 50, 100, 200, 300 most common words in poetry book titles. Ntree = 250, mtry = 10",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("25", "50", "100", "200", "300"),
x_tick_breaks = c(25, 50, 100, 200, 300),
legend_title_parentheses=c(""))
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry[0-9]+_whole_title_alt_ntree250_mtry10.*no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of BOW size alt",
main_title = "Effect of BOW size: Method 2",
sub_title = "Features: 25, 50, 100, 200, 300 most common words in poetry book titles. Ntree = 250, mtry = 10",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("25", "50", "100", "200", "300"),
x_tick_breaks = c(25, 50, 100, 200, 300),
legend_title_parentheses=c(""))
make_pic_comparison_lines(filepath=outputpath,
inputfile_patterns = c("poetry[0-9]+_title_only_alt_ntree250_mtry10.*no_cutoff.txt") ,
parameter_names = c("precision", "recall", "balanced_accuracy"),
outputfile = "Effect of BOW size alt title only",
main_title = "Effect of BOW size: Method 3",
sub_title = "Features: 25, 50, 100, 200, 300 most common words in poetry book titles. Ntree = 250, mtry = 10",
x_title = "",
legend_labels = c("Precision", "Recall", "Balanced accuracy"),
x_tick_labels = c("25", "50", "100", "200", "300"),
x_tick_breaks = c(25, 50, 100, 200, 300),
legend_title_parentheses=c(""))
source('~/GitHub/Gradu/make_all_pics.R')
feats_basic_bow18_sans_genreshares <- readRDS(paste0(bu_path, "/features_basic_bow18.RDS"))
names(feats_basic_bow18_sans_genreshares)
feats_bagofwords <- feats_poetry100[,c("poetry_alt_tune",
"poetry_alt_song",
"poetry_alt_poem",
"poetry_alt_songs",
"poetry_alt_poems",
"poetry_alt_sung",
"poetry_alt_love",
"poetry_alt_ballad",
"poetry_alt_poetical",
"poetry_alt_elegy",
"poetry_alt_ode",
"poetry_alt_maid",
"poetry_alt_garland",
"poetry_alt_lamentation",
"poetry_alt_hymn",
"poetry_alt_hymns",
"poetry_alt_epistle",
"poetry_alt_psalms",
"poetry_alt_verses")]
feats_bagofwords <- feats[,c("poetry_alt_tune",
"poetry_alt_song",
"poetry_alt_poem",
"poetry_alt_songs",
"poetry_alt_poems",
"poetry_alt_sung",
"poetry_alt_love",
"poetry_alt_ballad",
"poetry_alt_poetical",
"poetry_alt_elegy",
"poetry_alt_ode",
"poetry_alt_maid",
"poetry_alt_garland",
"poetry_alt_lamentation",
"poetry_alt_hymn",
"poetry_alt_hymns",
"poetry_alt_epistle",
"poetry_alt_psalms",
"poetry_alt_verses")]
feats_bagofwords <- feats[,c("poetry_alt_tune",
"poetry_alt_song",
"poetry_alt_poem",
"poetry_alt_songs",
"poetry_alt_poems",
"poetry_alt_sung",
"poetry_alt_love",
"poetry_alt_ballad",
"poetry_alt_poetical",
"poetry_alt_elegy",
"poetry_alt_ode",
"poetry_alt_maid",
"poetry_alt_garland",
"poetry_alt_lamentation",
"poetry_alt_hymn",
"poetry_alt_hymns",
"poetry_alt_epistle",
"poetry_alt_psalms",
"poetry_alt_verses")]
feats_bagofwords <- feats[,c("poetry_alt_tune",
"poetry_alt_song",
"poetry_alt_poem",
"poetry_alt_songs",
"poetry_alt_poems",
"poetry_alt_sung",
"poetry_alt_love",
"poetry_alt_ballad",
"poetry_alt_poetical",
"poetry_alt_elegy",
"poetry_alt_ode",
"poetry_alt_maid",
"poetry_alt_garland",
"poetry_alt_lamentation",
"poetry_alt_hymn",
"poetry_alt_hymns",
"poetry_alt_epistle",
"poetry_alt_psalms",
"poetry_alt_verses")]
feats_poetry100 <- feats
feats_bagofwords <- feats_poetry100[,c("poetry_alt_tune",
"poetry_alt_song",
"poetry_alt_poem",
"poetry_alt_songs",
"poetry_alt_poems",
"poetry_alt_sung",
"poetry_alt_love",
"poetry_alt_ballad",
"poetry_alt_poetical",
"poetry_alt_elegy",
"poetry_alt_ode",
"poetry_alt_maid",
"poetry_alt_garland",
"poetry_alt_lamentation",
"poetry_alt_hymn",
"poetry_alt_hymns",
"poetry_alt_epistle",
"poetry_alt_psalms",
"poetry_alt_verses")]
feats_poetry100 <- readRDS(paste0(bu_path, "/feats_poetry_whole_title_alt_100_20170531.RDS"))
feats_bagofwords <- feats_poetry100[,c("poetry_alt_tune",
"poetry_alt_song",
"poetry_alt_poem",
"poetry_alt_songs",
"poetry_alt_poems",
"poetry_alt_sung",
"poetry_alt_love",
"poetry_alt_ballad",
"poetry_alt_poetical",
"poetry_alt_elegy",
"poetry_alt_ode",
"poetry_alt_maid",
"poetry_alt_garland",
"poetry_alt_lamentation",
"poetry_alt_hymn",
"poetry_alt_hymns",
"poetry_alt_epistle",
"poetry_alt_psalms",
"poetry_alt_verses")]
feats_bagofwords$is_poetry <- feats_poetry100$is_poetry
saveRDS(feats_bagofwords, paste0(bu_path, "/bagofwords19.RDS"))
